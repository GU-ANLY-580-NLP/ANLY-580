{"assets":{"BC5E289E7DA97D7482D3D04C16142EB3":{"type":"texture","index":0,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/7194770C-409F-4A0B-95C2-FDFD58044A8F.pdf"},"width":1920,"height":1080},"8C007E3B3D9A1BD9F4983FA64C6612A3":{"type":"texture","index":1,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/7194770C-409F-4A0B-95C2-FDFD58044A8F.pdf"},"width":1920,"height":1080}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"8C007E3B3D9A1BD9F4983FA64C6612A3"},{"animations":[{"additive":false,"timeOffset":0,"beginTime":0,"from":{"scalar":false},"repeatCount":0,"fillMode":"both","duration":0.01,"autoreverses":false,"property":"hidden","to":{"scalar":true},"removedOnCompletion":false}],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"BC5E289E7DA97D7482D3D04C16142EB3"}]}]},"effects":[],"duration":0.01,"type":"transition","attributes":{"direction":0},"name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"pasted-image.pdf","targetRectangle":{"y":131.73075675964355,"x":1033.9537353714309,"width":740.6875,"height":788.53125}},{"text":"ï¿¼","targetRectangle":{"y":1038.7800302505493,"x":1894.5309820581315,"width":13.001999999999953,"height":26}},{"text":"Figure taken from Bloem, 2020","targetRectangle":{"y":912.99278151988983,"x":1537.9263162841798,"width":231.77599999999984,"height":21.839999675750732}},{"text":"Original paper: Attention is all you need (2017)","targetRectangle":{"y":142.06993103027344,"x":61.333332061767578,"width":809.46199999999999,"height":110}},{"text":"30K citations","targetRectangle":{"y":307.06993103027344,"x":61.333332061767578,"width":261.096,"height":55}},{"text":"First neural language model to process sequence without use of recurrent connections or convolutions.","targetRectangle":{"y":417.06993103027344,"x":61.333332061767578,"width":796.90400000000011,"height":165}},{"text":"Reached SOTA BLEU \/ ROUGE scores","targetRectangle":{"y":637.06993103027344,"x":61.333332061767578,"width":762.54199999999992,"height":55}},{"text":"NMT w\/encoder-decoder architecture","targetRectangle":{"y":747.06993103027344,"x":61.333332061767578,"width":757.80400000000009,"height":55}},{"text":"Uses position encodings","targetRectangle":{"y":857.06993103027344,"x":61.333332061767578,"width":494.86800000000005,"height":55}},{"text":"The first transformer (2017)","targetRectangle":{"y":4,"x":40,"width":863.16999999999996,"height":84}},{"text":"Vaswani et al., Attention is All You Need, 2017","targetRectangle":{"y":1053.0695027112961,"x":4.4276669323444366,"width":335.08800000000002,"height":21.839999675750732}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"BC5E289E7DA97D7482D3D04C16142EB3"}]}]}}]}