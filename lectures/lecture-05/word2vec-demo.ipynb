{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1842f9-e117-45c7-8024-a27a28a6a162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training a Word2Vec model on the Wikipedia page about the *United States*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8c073-1114-49dc-9d86-11ca27419626",
   "metadata": {},
   "source": [
    "### Grab the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e0d622-c73f-407a-836d-b95b3333b0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# r = requests.get(\"https://en.wikipedia.org/wiki/United_States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0373a967-13a4-41cf-8719-15da573999c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file saved from september, 2021\n",
    "with open(\"us-wiki-september-2021.html\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2ed0b-75c7-4c0c-bcde-db04821d75e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing pipeline (repurposed from Lab-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda56c86-770c-44f1-ab49-0a61a7c082f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/ml-base/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "pipeline_name = 'WikiUS'\n",
    "\n",
    "@Language.component(pipeline_name)\n",
    "def preprocess(doc):\n",
    "    doc = [token for token in doc if not token.is_punct]\n",
    "    doc = [token for token in doc if not token.is_stop]\n",
    "    doc = [token.text.lower().strip() for token in doc]\n",
    "    doc = [token for token in doc if 0 < len(token) <= 12]\n",
    "    return \" \".join(doc)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    # http://emailregex.com/\n",
    "    email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)\n",
    "    *|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\n",
    "    |\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]\n",
    "    (?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\n",
    "    (?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:\n",
    "    (?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "    # replace = [ (pattern-to-replace, replacement),  ...]\n",
    "    replace = [\n",
    "        (\"<[^>]*>\", \" \"),\n",
    "        (email_re, \" \"),                           # Matches emails\n",
    "        (r\"(?<=\\d),(?=\\d)\", \"\"),                   # Remove commas in numbers\n",
    "        (r\"\\d+\", \" \"),                             # Map digits to special token <numbr>\n",
    "        (r\"[*\\^\\.$&@<>,\\-/+{|}=?#:;'\\\"\\[\\]]\", \"\"), # Punctuation and other junk\n",
    "        (r\"[\\n\\t\\r]\", \" \"),                        # Removes newlines, tabs, creturn\n",
    "        (r\"[^\\x00-\\x7F]+\", \"\"),                    # Removes non-ascii chars\n",
    "        (r\"\\\\+\", \" \"),                             # Removes double-backslashs\n",
    "        (r\"\\s+n\\s+\", \" \"),                         # 'n' leftover from \\\\n\n",
    "        (r\"\\s+\", \" \")                              # Strips extra whitespace\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = spacy.load('en_core_web_sm')\n",
    "        self.pipeline.add_pipe(pipeline_name);\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.transform(*args, **kwargs)\n",
    "\n",
    "    def transform(self, doc: str):\n",
    "        for repl in self.replace:\n",
    "            doc = re.sub(repl[0], repl[1], doc)\n",
    "        return self.pipeline(doc)\n",
    "\n",
    "\n",
    "pipeline = Pipeline();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b3e80-d257-477e-b87e-cecf0261d4ba",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5dd659-bff9-4aa1-b4ae-e41e2d7a0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "i = 0\n",
    "while i < len(text):\n",
    "    text_chunk = text[i:i + int(1e5)]\n",
    "    docs.append(pipeline(text_chunk))\n",
    "    i += int(1e5)\n",
    "doc = \" \".join(docs);\n",
    "\n",
    "# Wiki articles have many citations\n",
    "doc = doc.replace('citeref', '').replace(\"tmulti\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf074-3aa9-4c4c-9dae-ade5f4e99827",
   "metadata": {},
   "source": [
    "### Excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881aad54-8cf4-4179-be36-8b7d639a0b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ossing worldwide sales s bob dylan emerged folk revival americas celebrated songwriters james brown led development funk recent american creations include hip hop salsa techno house music mid thcentury american pop stars bing crosby frank sinatra elvis presley global celebrities artists late th century michael jackson prince madonna whitney houston popular artists mid s late s include mariah carey britney spears justin timberlake christina aguilera beyonc wellknown american singers s include katy perry bruno mars lady gaga taylor swift ariana grande cinema main article cinema united states hollywood sign los angeles california hollywood northern district los angeles california leaders motion picture production worlds commercial motion picture exhibition given new york city thomas edison s kinetoscope early th century film industry largely based hollywood st century increasing number films film companies subject forces director d w griffith american filmmaker silent film period central development film grammar walt disney leader animated film movie directors john ford redefined image american old west like john huston broadened cinema location shooting industry enjoyed golden years commonly referred golden age hollywood early sound period early s screen actors john wayne marilyn monroe iconic figures s new hollywood hollywood renaissance defined grittier films influenced french italian realist pictures postwar period recent times directors steven spielberg george lucas james cameron gained renown blockbuster films high production costs earnings notable films topping american film institute s afi list include orson welles s citizen kane frequently cited greatest film time casablanca godfather gone wind lawrence arabia wizard oz graduate waterfront schindlers list singin rain wonderful life sunset boulevard academy awards popularly known oscars held annually academy motion picture arts sciences golden globe awards held annually january sports main article sports united states popular sports american football basketball baseball ice hockey american football measures popular spectator sport united states national football league nfl highest average attendance sports league world super bowl watched tens millions globally collegiate level college football games receive millions viewers television broadcast notably college football playoff averages million viewers baseball regarded national sport late th century major league baseball mlb league basketball ice hockey countrys leading professional team sports leagues national basketball association nba national hockey league nhl college football basketball attract large audiences ncaa final watched sporting events soccer sport gained footing united states mid s country hosted fifa world cup mens national soccer team qualified world cups womens team won fifa womens world cup times major league soccer sports highest league united states featuring american canadian teams market professional sports united states roughly billion roughly larger europe middle east africa combined olympic games taken place united states summer olympics st louis missouri firstever olympic games held outside europe update united states won medals summer olympic games country winter olympic games second norway major sports baseball american football evolved european practices basketball volleyball snowboarding american inventions popular worldwide lacrosse surfing arose native american native hawaiian activities predate western contact mostwatched individual sports golf auto racing particularly nascar indycar mass media information mass media united states newspapers united states television united states internet united states radio united states headquarters national broadcasting company nbc rockefeller plaza new york city major broadcasters national broadcasting company nbc columbia broadcasting system cbs american broadcasting company abc fox broadcasting company fox major broadcast television networks commercial entities cable television offers hundreds channels catering variety niches americans listen radio programming largely commercial average twoandahalf hours day number commercial radio stations grown stations fm stations addition public radio stations stations run universities public authorities educational purposes financed public private funds corporate underwriting public radio broadcasting supplied npr npr incorporated february public broadcasting act television counterpart pbs created legislation update licensed fullpower radio stations according federal commission fcc wellknown newspapers include wall street journal new york times usa today cost publishing increased years price newspapers generally remained low forcing newspapers rely advertising revenue articles provided major wire service associated press reuters national world coverage exceptions newspapers privately owned large chains gannett mcclatchy dozens hundreds newspapers small chains handful papers situation increas'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[65000:70000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178caea-6edc-4861-bd23-ddc1d82f2eff",
   "metadata": {},
   "source": [
    "### Construct a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe578e7-da7b-48a3-9ca8-411ee03d2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = doc.split(\" \")\n",
    "words_histogram = Counter(doc)\n",
    "\n",
    "freq_threshold = 2\n",
    "vocab = {}\n",
    "for word, count in sorted(words_histogram.items(), \n",
    "                          key=lambda wrd_cnt: wrd_cnt[0]):\n",
    "    if word and count >= freq_threshold:\n",
    "        vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887ca20-e696-4229-a3b2-6df260a77940",
   "metadata": {},
   "source": [
    "### Remove OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49040993-ad1f-4a9c-a185-85c9ca1b0445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = [word for word in doc if vocab.get(word) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d7563d-ad27-4176-9a4b-ec5585e04b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16317, 2364)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "N = len(vocab)\n",
    "\n",
    "# Training set size\n",
    "M = len(doc)\n",
    "\n",
    "M, N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921216f1-aa8d-4ce9-a6e9-c8f54d30a121",
   "metadata": {},
   "source": [
    "# Build a word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1696d-4288-4144-b77b-aaebe148616f",
   "metadata": {},
   "source": [
    "### Softmax (numerically stable version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7eb309-567b-4d00-884f-fecbb33a8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z) -> np.ndarray:\n",
    "    Z_exp = np.exp(Z - np.max(Z))\n",
    "    partition = np.sum(Z_exp)\n",
    "    return Z_exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f07786-3439-4cb5-9757-0700d7a6abb1",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b5203d-11b2-4bd8-8110-c9c0c75a85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding dimension (K << N)\n",
    "K = 15\n",
    "\n",
    "# Number of passes through the data\n",
    "epochs = 65\n",
    "\n",
    "# The number of words on both sides\n",
    "# of center word to consider as \n",
    "# context\n",
    "context_window = 6\n",
    "\n",
    "# Learning rate for gradient updates\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a8f89-4f17-4c6f-860d-33d2974d10ab",
   "metadata": {},
   "source": [
    "### Construct a projection matrix shape = (K x N)\n",
    "\n",
    "Note: W is our projection matrix, w represents a word (these are different things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67cd22e5-8bc1-4528-bf97-ede957637bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding():\n",
    "    \"\"\"\n",
    "    Make a randomly initialized embedding \n",
    "    matrix of shape K x N\n",
    "    \"\"\"\n",
    "    return np.random.random((K, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d91d1-4699-47e2-be91-68df85b3727d",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da20395-1321-4d22-b408-d8fce06eeed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 65, ce-loss: 65.6998, acc: 0.5559: 100%|██████████| 1059825/1059825 [19:20<00:00, 913.58it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Word embeddings\n",
    "U = initialize_embedding()\n",
    "\n",
    "# Context embeddings\n",
    "V = initialize_embedding()\n",
    "\n",
    "# One-hot encoded word representations\n",
    "X = np.eye(N)\n",
    "\n",
    "with tqdm(total=int(epochs * (M - 2 * context_window))) as bar:\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "        ce_losses = []\n",
    "        acc = []\n",
    "        \n",
    "        for i in range(context_window, M - context_window, 1):\n",
    "            \n",
    "            word_doc_idx = i\n",
    "            word = doc[word_doc_idx]\n",
    "            word_idx = vocab.get(word, 0)\n",
    "            \n",
    "            context_words = doc[i - context_window: i + context_window + 1]\n",
    "            context_words = np.delete(context_words, context_window)\n",
    "            context_idxs = [vocab.get(context_word, 0) for context_word in context_words]        \n",
    "            \n",
    "            # print(word, context_words)\n",
    "            \n",
    "            # One-hot encoded center word (1 x N)\n",
    "            x_w = X[word_idx]\n",
    "            \n",
    "            # One-hot encoded context words (2 * context window x N)\n",
    "            X_c = X[context_idxs]\n",
    "            \n",
    "            # Embedding for word w (1 x K)\n",
    "            u_w = U[:, word_idx]\n",
    "            \n",
    "            # Inner product between embedding for word w and embedding for all context words in V (1 x N)\n",
    "            Z = u_w.dot(V)\n",
    "            \n",
    "            # Probability distribution over all context words for center word w (1 x N)\n",
    "            P = softmax(Z)\n",
    "            \n",
    "            # Cross entropy loss\n",
    "            ce = -np.sum(np.log(P[context_idxs]))\n",
    "            ce_losses.append(ce)\n",
    "            \n",
    "            # Prediction errors (2 * context_window x N)\n",
    "            errors = P - X_c\n",
    "            \n",
    "            for error in errors:\n",
    "                \n",
    "                # Gradients w.r.t. U_w: dNLL/dU_w = sum(V.dot(P - X_c))\n",
    "                grad_U_w = V.dot(error.T) # (K x 1)\n",
    "\n",
    "                # Gradient w.r.t. V: dNLL/dV = u_w x P - X_c\n",
    "                grad_V = np.expand_dims(u_w, axis=1).dot(np.expand_dims(error, axis=0))\n",
    "                \n",
    "                # Gradient updates\n",
    "                U[:, word_idx] -= lr * grad_U_w.T\n",
    "                V -= lr * grad_V\n",
    "            \n",
    "            # Compute accuracy of context word being in top-50 from softmax probability\n",
    "            top = np.argsort(P)[-50:]\n",
    "            acc.append(sum([1 for idx in context_idxs if idx in top]) / (2 * context_window))\n",
    "            \n",
    "            bar.update()\n",
    "            \n",
    "        bar.set_description(\"epoch: %d, ce-loss: %.4f, acc: %.4f\" %\n",
    "                    (epoch + 1, np.mean(ce_losses), np.mean(acc)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135a5ac-14bc-443e-9c94-6f4065fb4761",
   "metadata": {},
   "source": [
    "### Compute most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0c8330-3a3a-4a9f-ab65-9a761d440716",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlookup = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "Unormed = U / np.expand_dims(np.linalg.norm(U, axis=0, ord=2), axis=0)\n",
    "\n",
    "def compute_top_n_similar_words(word, n=20, method='dot'):\n",
    "    idx = vocab[word]\n",
    "    if method == 'dot':\n",
    "        neighbors_idxs = np.argsort(U[:, idx].dot(U))[-n:][::-1]\n",
    "    elif method == 'cosine':\n",
    "        neighbors_idxs = np.argsort(Unormed[:, idx].dot(Unormed))[-n:][::-1]\n",
    "    neighbors = [wordlookup[idx] for idx in neighbors_idxs]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e5a8039-d438-4387-9c85-3b2ea3e79579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "budget :  ['budget', 'fiscal', 'spent', 'office', 'federal', 'reserve', 'municipal', 'currency', 'department', 'amounted']\n",
      "cia :  ['cia', 'factbook', 'federal', 'intelligence', 'agency', 'vehicle', 'clock', 'central', 'ciagov', 'accounting']\n",
      "virginia :  ['virginia', 'gazette', 'williamsburg', 'jefferson', 'wisconsin', 'nbspjun', 'north', 'utah', 'al', 'peter']\n",
      "suffrage :  ['suffrage', 'prohibition', 'womens', 'saw', 'movement', 'measures', 'led', 'abolition', 'legislation', 'prominent']\n",
      "civil :  ['civil', 'progressive', 'charles', 'branch', 'rights', 'era', 'slavery', 'movement', 'ultimately', 'republicans']\n",
      "war :  ['war', 'cold', 'vietnam', 'ii', 'gorbachev', 'ussoviet', 'terror', 'iraq', 'tallest', 'end']\n",
      "election :  ['election', 'presidential', 'republican', 'joe', 'party', 'president', 'democrat', 'elected', 'biden', 'founded']\n",
      "declaration :  ['declaration', 'independence', 'continental', 'draft', 'written', 'wilson', 'founding', 'committee', 'lee', 'britain']\n",
      "president :  ['president', 'vice', 'presidential', 'tempore', 'elected', 'election', 'veto', 'override', 'senate', 'kamala']\n",
      "obama :  ['obama', 'barack', 'ensure', 'stimulus', 'multiracial', 'crisis', 'face', 'announces', 'elected', 'measures']\n",
      "trump :  ['trump', 'supporters', 'capitol', 'biden', 'harris', 'baseball', 'football', 'announces', 'president', 'elected']\n",
      "democratic :  ['democratic', 'party', 'republican', 'blue', 'parties', 'subsequent', 'admitted', 'founded', 'conventions', 'consists']\n",
      "republican :  ['republican', 'democratic', 'party', 'election', 'parties', 'blue', 'founded', 'elections', 'subsequent', 'ultimately']\n",
      "baseball :  ['baseball', 'football', 'sport', 'ice', 'basketball', 'audiences', 'hockey', 'league', 'lead', 'nba']\n",
      "west :  ['west', 'plains', 'colorado', 'eastern', 'south', 'mountains', 'humid', 'meridian', 'morrison', 'location']\n",
      "vice :  ['vice', 'kamala', 'president', 'speaker', 'leadership', 'joe', 'tempore', 'pelosi', 'senate', 'harris']\n",
      "speaker :  ['speaker', 'pelosi', 'leadership', 'vice', 'tempore', 'nancy', 'kamala', 'senate', 'chief', 'house']\n",
      "white :  ['white', 'canyon', 'house', 'elected', 'multiracial', 'policies', 'presidential', 'seats', 'ancestry', 'deal']\n",
      "iraq :  ['iraq', 'afghanistan', 'egypt', 'tallest', 'terror', 'iran', 'saudi', 'kuwait', 'end', 'launched']\n",
      "games :  ['games', 'olympic', 'norway', 'medals', 'sports', 'media', 'won', 'europe', 'video', 'africa']\n",
      "supreme :  ['supreme', 'court', 'decision', 'ruling', 'judicial', 'striking', 'serves', 'justice', 'chief', 'appointed']\n",
      "abortion :  ['abortion', 'obesity', 'gun', 'health', 'insurance', 'rates', 'care', 'lost', 'high', 'smoking']\n",
      "left :  ['left', 'paris', 'joined', 'confederacy', 'election', 'viewed', 'agreement', 'power', 'colony', 'climate']\n",
      "right :  ['right', 'compact', 'liberty', 'federation', 'bicameral', 'regulated', 'extremely', 'rule', 'founding', 'calling']\n",
      "senate :  ['senate', 'house', 'approval', 'vice', 'president', 'legislative', 'members', 'congress', 'speaker', 'officers']\n",
      "house :  ['house', 'senate', 'pro', 'speaker', 'policies', 'seats', 'documents', 'leadership', 'white', 'pelosi']\n"
     ]
    }
   ],
   "source": [
    "words = ['budget', 'cia', 'virginia', 'suffrage', 'civil', \n",
    "         'war', 'election', 'declaration', 'president', 'obama', \n",
    "         'trump', 'democratic', 'republican', 'baseball', 'west',\n",
    "         'vice', 'speaker', 'white', 'iraq', 'games', 'supreme',\n",
    "         'abortion', 'left', 'right', 'senate', 'house']\n",
    "\n",
    "for word in words:\n",
    "    print(word, \": \", compute_top_n_similar_words(word, n=10, method='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48b7f6-df40-4c24-8fdb-3af8e5226bfc",
   "metadata": {},
   "source": [
    "### Compute analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e0462d-a252-4715-9eeb-6a5380b32b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analogy(word1, word2, word3):\n",
    "    \"\"\"\n",
    "    Desired behavior:\n",
    "        word2 - word1 + word3 = word4\n",
    "        king - man + woman = queen\n",
    "    \"\"\"\n",
    "    u1 = Unormed[:, vocab[word1]]\n",
    "    u2 = Unormed[:, vocab[word2]]\n",
    "    u3 = Unormed[:, vocab[word3]]\n",
    "    u4 = u2 - u1 + u3\n",
    "    u4_idxs = np.argsort(u4.dot(Unormed))[-10:][::-1]\n",
    "    word4_candidates = [wordlookup[idx] for idx in u4_idxs]\n",
    "    return word4_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b87da5-6041-4349-b5c2-5165c3915b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'senate', 'house', 'v', 'nancy', 'chief', 'legislative', 'officers', 'px', 'pelosi']\n",
      "['vice', 'leadership', 'kamala', 'pelosi', 'includes', 'nancy', 'senate', 'speaker', 'president', 'chief']\n",
      "['jefferson', 'wrote', 'virginia', 'england', 'moylan', 'evidence', 'encounters', 'williamsburg', 'inhabitants', 'migrated']\n"
     ]
    }
   ],
   "source": [
    "analogies = [('mitch', 'senate', 'nancy'),\n",
    "             ('obama', 'president', 'kamala'),\n",
    "             ('adams', 'england', 'jefferson')]\n",
    "\n",
    "for analogy in analogies:\n",
    "    print(compute_analogy(*analogy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ce81e-dd4c-4d04-aaba-9eb5155a37e4",
   "metadata": {},
   "source": [
    "# Visualize embeddings using Tensorboard Projector\n",
    "\n",
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab694e59-6b5a-448d-8adc-09124e8a0522",
   "metadata": {},
   "source": [
    "### Save embeddings/vocab to disk\n",
    "\n",
    "Use the cell below to save your embeddings and vocab to disk (`vectors.tsv`, `metadata.tsv`). The embedding projector expects data in the following format:\n",
    "\n",
    "\n",
    "\n",
    "`vectors.tsv` (N=3 K=4 embeddings):\n",
    "\n",
    "    0.1\\t0.2\\t0.5\\t0.9\n",
    "    0.2\\t0.1\\t5.0\\t0.2\n",
    "    0.4\\t0.1\\t7.0\\t0.8\n",
    "\n",
    "\n",
    "`metadata.tsv` (N=3 word vocabulary):\n",
    "\n",
    "    three\n",
    "    word\n",
    "    vocabulary\n",
    "\n",
    "You can use the helper functions in the cell below to save and load embeddings/vocab to/fro disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8c800ff-8d3b-42e1-87e5-b0dc8272c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matrix(matrix, fpath):\n",
    "    D1, D2 = matrix.shape\n",
    "    tsv = \"\"\n",
    "    for i in range(D1):\n",
    "        for j in range(D2):\n",
    "            tsv += str(matrix[i, j]) + '\\t'\n",
    "        tsv = tsv.strip('\\t') + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)\n",
    "\n",
    "        \n",
    "def load_matrix(fpath):\n",
    "    matrix = []\n",
    "    with open(fpath, 'r') as fd:\n",
    "        tsv = fd.read()\n",
    "    for line in tsv.split('\\n'):\n",
    "        row = []\n",
    "        for value in line.split('\\t'):\n",
    "            row.append(float(value))\n",
    "        matrix.append(row)\n",
    "    return np.array(matrix)\n",
    "\n",
    "        \n",
    "def save_vocab(vocab: dict, fpath):\n",
    "    tsv = \"\"\n",
    "    for word, idx in sorted(vocab.items(), key=lambda item: item[1]):\n",
    "        tsv += word + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)\n",
    "\n",
    "\n",
    "def load_vocab(fpath):\n",
    "    with open(fpath, \"r\") as fd:\n",
    "        tsv = fd.read()\n",
    "    vocab = {}\n",
    "    for line in tsv.split('\\n'):\n",
    "        vocab[line.strip()] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9e4840a-26a2-4f85-a19f-b5c81edff4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2364, (15, 2364))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_matrix(U.T, \"vectors.tsv\")\n",
    "save_vocab(vocab, \"metadata.tsv\")\n",
    "\n",
    "len(load_vocab(\"metadata.tsv\")), load_matrix(\"vectors.tsv\").T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d387b12-cc58-4ebe-892b-723b75a171c5",
   "metadata": {},
   "source": [
    "### Instructions for uploading data into the Projector\n",
    "\n",
    "Click the load button called *load* on the left side of the screen. You will then be given the option to upload two files (image below). Upload your `vectors.tsv` and `metadata.tsv` files from the last step and then click out of the pop up (oddly the pop-up does not close once your files have been uploaded).\n",
    "\n",
    "<img src=\"projector-load.png\" alt=\"Embedding/Vocab upload\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a1a61-606c-44f3-b8ff-a26d9b769f1b",
   "metadata": {},
   "source": [
    "### Instructions for using the Projector\n",
    "\n",
    "There are three compression algorithms that you can use, I recommend tSNE with the default settings. After ~2000 iterations tSNE should yield words clustered in a way that loosly reflects underlying semantic relationships. There is a search bar on the right; baked into the UI is a entity tagger that automatically tags words from `metadata.tsv` with entities (if one is recognized). When you type in the name of a state for example, the Projector will highlight the other words tagged with <STATE>, and you will a tight clustering of states in embeddings space (below).\n",
    "\n",
    "<img src=\"projector.png\" alt=\"Embedding projector snapshot\" width=\"1000\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaf90f38dd02d397fb672f3eedea9f67f57967a2a80a762c925841e1b54a5d86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
